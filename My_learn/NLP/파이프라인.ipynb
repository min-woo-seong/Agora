{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cc74319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.24.0-py3-none-any.whl (5.5 MB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\sba13\\anaconda3\\lib\\site-packages (from transformers) (2022.3.15)\n",
      "Collecting huggingface-hub<1.0,>=0.10.0\n",
      "  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.2-cp39-cp39-win_amd64.whl (3.3 MB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\sba13\\anaconda3\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: requests in c:\\users\\sba13\\anaconda3\\lib\\site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\sba13\\anaconda3\\lib\\site-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: filelock in c:\\users\\sba13\\anaconda3\\lib\\site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\sba13\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\sba13\\anaconda3\\lib\\site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\sba13\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\sba13\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\sba13\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.4)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\sba13\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sba13\\anaconda3\\lib\\site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sba13\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\sba13\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.9)\n",
      "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.10.1 tokenizers-0.13.2 transformers-4.24.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54ba3b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b091a117",
   "metadata": {},
   "source": [
    "## 질문 답변"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bed68dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35985da586ef4a0d90d97fbceed84eb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/473 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SBA13\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:123: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\SBA13\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfa6daaf120147e88faac75236a2c690",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/261M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFDistilBertForQuestionAnswering.\n",
      "\n",
      "All the layers of TFDistilBertForQuestionAnswering were initialized from the model checkpoint at distilbert-base-cased-distilled-squad.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForQuestionAnswering for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f27a76a9a7a54802b5409f483a05c2be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "013ed036168740648e7066d5b7d8bf48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fbdb999d6324da7a347f059ce82f6b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "qa = pipeline(\"question-answering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b3908ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 지문\n",
    "context = \"\"\"\n",
    "Seoul,  officially known as the Seoul Special City, \n",
    "is the capital and largest metropolis of South Korea.\n",
    "According to the 2020 census, Seoul has a population of 9.9 million people, \n",
    "and forms the heart of the Seoul Capital Area with the surrounding Incheon metropolis \n",
    "and Gyeonggi province.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80474608",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.8752999305725098, 'start': 1, 'end': 6, 'answer': 'Seoul'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 질문: 한국의 수도는?\n",
    "qa(question=\"where is the capital city of South Korea?\", context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "236d0b4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.9284715056419373,\n",
       " 'start': 164,\n",
       " 'end': 175,\n",
       " 'answer': '9.9 million'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 질문: 서울에 사는 사람은 몇 명?\n",
    "qa(question=\"How many people live in Seoul?\", context=context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14013450",
   "metadata": {},
   "source": [
    "## NER(Named Entity Recognition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1814c38d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe22e5bea25d4b859ed7c35dbb3357e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/998 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08226eba30a14c4f943a4af4f0934be2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing TFBertForTokenClassification: ['dropout_147']\n",
      "- This IS expected if you are initializing TFBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForTokenClassification were initialized from the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForTokenClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25c2b18cd2544403873fca770d7bc79d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/60.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba1cf13a3228482b93b330d24bf5ae7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ner = pipeline(\"ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78f163bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'I-LOC',\n",
       "  'score': 0.9995431,\n",
       "  'index': 1,\n",
       "  'word': 'Seoul',\n",
       "  'start': 1,\n",
       "  'end': 6},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.9981414,\n",
       "  'index': 7,\n",
       "  'word': 'Seoul',\n",
       "  'start': 33,\n",
       "  'end': 38},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.9888005,\n",
       "  'index': 8,\n",
       "  'word': 'Special',\n",
       "  'start': 39,\n",
       "  'end': 46},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.9927395,\n",
       "  'index': 9,\n",
       "  'word': 'City',\n",
       "  'start': 47,\n",
       "  'end': 51},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.99765164,\n",
       "  'index': 19,\n",
       "  'word': 'South',\n",
       "  'start': 95,\n",
       "  'end': 100},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.99936754,\n",
       "  'index': 20,\n",
       "  'word': 'Korea',\n",
       "  'start': 101,\n",
       "  'end': 106},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.9993856,\n",
       "  'index': 28,\n",
       "  'word': 'Seoul',\n",
       "  'start': 138,\n",
       "  'end': 143},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.9769224,\n",
       "  'index': 45,\n",
       "  'word': 'Seoul',\n",
       "  'start': 212,\n",
       "  'end': 217},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.94968396,\n",
       "  'index': 46,\n",
       "  'word': 'Capital',\n",
       "  'start': 218,\n",
       "  'end': 225},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.8967128,\n",
       "  'index': 47,\n",
       "  'word': 'Area',\n",
       "  'start': 226,\n",
       "  'end': 230},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.9985183,\n",
       "  'index': 51,\n",
       "  'word': 'Inc',\n",
       "  'start': 252,\n",
       "  'end': 255},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.9812338,\n",
       "  'index': 52,\n",
       "  'word': '##he',\n",
       "  'start': 255,\n",
       "  'end': 257},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.99869895,\n",
       "  'index': 53,\n",
       "  'word': '##on',\n",
       "  'start': 257,\n",
       "  'end': 259},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.9983987,\n",
       "  'index': 57,\n",
       "  'word': 'G',\n",
       "  'start': 276,\n",
       "  'end': 277},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.9893941,\n",
       "  'index': 58,\n",
       "  'word': '##ye',\n",
       "  'start': 277,\n",
       "  'end': 279},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.9963444,\n",
       "  'index': 59,\n",
       "  'word': '##ong',\n",
       "  'start': 279,\n",
       "  'end': 282},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.9953667,\n",
       "  'index': 60,\n",
       "  'word': '##gi',\n",
       "  'start': 282,\n",
       "  'end': 284}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fbc381",
   "metadata": {},
   "source": [
    "## 요약"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef7bf58a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to t5-small and revision d769bba (https://huggingface.co/t5-small).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09250720854841778f99e2fd83d2c02b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a000199de3724eb286004f9b6feb1845",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the layers of TFT5ForConditionalGeneration were initialized from the model checkpoint at t5-small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bc217b6b5064452b58bfc0c60a5a453",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f1022feb24b4b3286c8c912479fd750",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SBA13\\anaconda3\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5_fast.py:156: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "summ = pipeline(\"summarization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "238727ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "A zettelkasten consists of many individual notes with ideas and other short\n",
    "pieces of information that are taken down as they occur or are acquired. The\n",
    "notes are numbered hierarchically, so that new notes may be inserted at the\n",
    "appropriate place, and contain metadata to allow the note-taker to associate\n",
    "notes with each other. For example, notes may contain tags that describe key\n",
    "aspects of the note, and they may reference other notes. The numbering,\n",
    "metadata, format and structure of the notes is subject to variation depending on\n",
    "the specific method employed. Creating and using a zettelkasten is made easier\n",
    "by taking the notes down digitally and using appropriate knowledge management\n",
    "software. But it can be and has long been done on paper using index cards. The\n",
    "method not only allows a researcher to store and retrieve information related to\n",
    "their research, but also intends to enhance creativity. Cross-referencing notes\n",
    "through tags allows the researcher to perceive connections and relationships\n",
    "between individual items of information that may not be apparent in isolation.\n",
    "These emergent aspects of the method make the zettelkasten somewhat similar to a\n",
    "neural network with which one may \"converse\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da101f79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'a zettelkasten consists of many individual notes with ideas and short pieces of information . the notes are numbered hierarchically, so that new notes may be inserted at the appropriate place . notes may contain tags that describe key aspects of the note .'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summ(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedec09c",
   "metadata": {},
   "source": [
    "## Zero-Shot Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e958e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to roberta-large-mnli and revision 130fb28 (https://huggingface.co/roberta-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b200827cad3b4c74ba553833961e8edb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/688 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0327068cc564a80b7f949c2a3978568",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.43G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "zs = pipeline('zero-shot-classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364e7363",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = 'Pizza is my favorite food'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29db30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 레이블\n",
    "label = ['food', 'ocean', 'space']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff278cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분류\n",
    "zs(sequence, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f719f9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = ['politics', 'society', 'life']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2307daba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분류\n",
    "zs(sequence, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9426724e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
